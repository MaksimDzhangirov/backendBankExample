# Как развернуть веб-приложение в Kubernetes кластере на AWS EKS

[Оригинал](https://www.youtube.com/watch?v=PH-Mcd0Rs1w)

Всем привет, добро пожаловать на мастер-класс по бэкенду.

На предыдущих лекциях мы научились создавать EKS кластер на AWS и 
подключаться к нему с помощью `kubectl` или `k9s`.

Сегодня давайте узнаем как развернуть API сервис нашего простого банковского
приложения в этом Kubernetes кластере. Итак, мы создали Docker образ для 
этого сервиса и отправили его в Amazon ECR, а теперь хотим запустить этот 
образ как контейнер в Kubernetes кластере. Сделать это можно с помощью 
процесса развертывания.

## Процесс развертывания

Процесс развертывания заключается в описании того как мы хотим, чтобы наш
образ был развернут. Подробнее об этом можно прочитать на [официальной 
странице документации Kubernetes](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)

А вот пример типичного для Kubernetes файла процесса развертывания.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
```

Итак, давайте скопируем его содержимое и откроем проект нашего простого 
банковского приложения. Затем в папке `eks` я создам новый файл с названием
`deployment.yaml` и вставлю в него содержимое из примера.

В первой строке указана версия Kubernetes API, которую мы используем для 
создания этого объекта. Затем во второй строке указывается тип объекта, 
который мы хотим создать, в данном случае это `Deployment`.

```yaml
apiVersion: apps/v1
kind: Deployment
```

Далее идет раздел метаданных, где мы можем указать некоторые метаданные 
для объекта. Например, название объекта, я назову его 
`simple-bank-api-deployment`. А метки — это, по сути, некоторые пары 
ключ-значение, связанные с объектом, которые может использовать пользователь,
чтобы было проще управлять и выбирать множество объектов. Здесь я добавлю 
только одну метку для `app`, а именно `simple-bank-api`.

```yaml
metadata:
  name: simple-bank-api-deployment
  labels:
    app: simple-bank-api
```

Потом идёт основная спецификация объекта развертывания. Во-первых, мы можем
задать количество реплик или количество подов, которые мы хотим запустить с 
одним и тем же `template`. А пока давайте запустим только один под.

```yaml
spec:
  replicas: 1
```

Далее нам нужно указать как выбирать поды для этого развертывания. По сути, это 
правило, определяющее, как процесс развертывания может найти поды, которыми
нужно управлять. В этом случае мы применим правило `matchLabels`. И я буду 
использовать ту же метку приложения: `simple-bank-api`, которую задал ранее.

```yaml
spec:
  selector:
    matchLabels:
      app: simple-bank-api
```

Это означает, что все поды с этой меткой будут управляться этим процессом 
развёртывания.

Таким образом в следующем разделе `template` мы должны добавить такую же 
метку к его метаданным.

```yaml
spec:
  template:
    metadata:
      labels:
        app: simple-bank-api
```

Далее идёт спецификация пода. Здесь мы сообщаем процессу развертывания как
развертывать наши контейнеры. Во-первых, название контейнера должно быть
`simple-bank-api`. Затем URL-адрес, откуда нужно извлечь образ. Поскольку
наши образы `simple-bank` хранятся в Amazon ECR, давайте откроем его в 
браузере, чтобы получить URL-адрес.

![](../images/part32/1.png)

Здесь мы видим, что существует несколько образов с разными тегами.

![](../images/part32/2.png)

Я выберу самый последний и скопирую URL-адрес его образа, нажав на эту 
кнопку.

![](../images/part32/3.png)

Затем вставлю его в наш файл `deployment.yaml`.

```yaml
spec:
  containers:
    - name: simple-bank-api
      image: 095420225348.dkr.ecr.eu-west-1.amazonaws.com/simplebank:25d22b979a8876906cdbf57b16aa92d265ee46fb
```

Обратите внимание, что этот длинный суффикс URL является тегом образа.
И это по сути хэш Git коммита, как мы задали в одной из предыдущих лекций.
На данный момент мы прописываем это значение вручную, но не 
волнуйтесь, на следующих лекциях я покажу вам, как изменить его автоматически 
с помощью Github Actions CI/CD.

Хорошо, теперь последнее, что мы собираемся сделать, это указать порт 
контейнера. Это порт, через который можно будет взаимодействовать с 
контейнером по сети извне. Хотя это совершенно необязательно, рекомендуется 
указать этот параметр, поскольку он поможет вам или другим людям лучше понять 
конфигурацию развертывания. Итак, думаю, что параметров, которые мы задали, 
будет достаточно для работы.

```yaml
    spec:
      containers:
        - name: simple-bank-api
          image: 095420225348.dkr.ecr.eu-west-1.amazonaws.com/simplebank:latest
          ports:
            - containerPort: 8080
```

Файл процесса развертывания готов. Но прежде чем мы его добавим, давайте 
воспользуемся `k9s`, чтобы проверить текущее состояние EKS кластера, который 
мы настроили на AWS в предыдущих лекциях. Если мы выберем пространство 
имен по умолчанию,

![](../images/part32/4.png)

то увидим, что на данный момент в нём нет запущенных подов.

![](../images/part32/5.png)

И список развертываний также пуст.

![](../images/part32/6.png)

Теперь давайте добавим файл развертывания, который мы создали ранее, используя
команду `kubectl apply`. Мы воспользуемся ключом `-f`, чтобы указать путь к 
объекту, который мы хотим добавить, в данном случае, это файл 
`deployment.yaml` внутри папки `eks`.

```shell
kubectl apply -f eks/deployment.yaml
deployment.apps/simple-bank-api-deployment created
```

Хорошо, команда успешно выполнена и процесс развертывания `simple-bank-api`
было создано. Давайте проверим так ли это через консоль `k9s`. 

![](../images/part32/7.png)

Вот, как видите развертывание появилось в списке, но по какой-то причине
оно ещё не готово к использованию. Чтобы получить больше информации, мы
можем нажать `d`, чтобы просмотреть этот объект развертывания. Итак, 
URL-адрес изображения правильный.

![](../images/part32/8.png)

И в списке `Events` видно сообщение о том, что число реплик 
`simple-bank-api-deployment` увеличено до 1. Все выглядит вполне нормально.
Но почему тогда развертывание не готово к использованию. Давайте нажмем
`Enter`, чтобы открыть список подов, которыми управляет это развертывание.

![](../images/part32/9.png)

Итак, похоже, что под ещё не готов. Его статус по-прежнему `Pending` («В 
режиме ожидания»). Давайте просмотрим его, чтобы получить больше информации.

![](../images/part32/10.png)

Если мы прокрутим вниз, чтобы увидеть список `Events`, то мы увидим 
предупреждающее событие: `FailedScheduling`. Оно связано с тем, что нет 
доступных узлов для распределения подов. Итак, теперь когда мы знаем причину, 
давайте перейдем на страницу AWS консоли и откроем кластер EKS
`simple-bank`, который мы настроили в предыдущих лекциях.

![](../images/part32/11.png)

Вуаля, здесь написано "This cluster does not have any attached
nodes" («У этого кластера нет подключенных к нему узлов»). Откроем вкладку
`Configuration` и выберем раздел `Compute`.

![](../images/part32/12.png)

В таблице `Node Groups` мы видим, что желаемое количество равен 0, поэтому 
не было создано ни одного узла (или EC2 инстансов). Чтобы исправить это, 
давайте откроем группу узлов `simple-bank`.

![](../images/part32/13.png)

Здесь мы видим,

![](../images/part32/14.png)

что желаемое количество для неё равно 0, как и минимальное количество.
Давайте нажмем эту кнопку `Edit`, чтобы изменить его. Я собираюсь увеличить 
желаемое количество до 1. Обратите внимание, что это число должно быть в 
пределах диапазона минимального и максимального количества. Итак, давайте
нажмем `Update`.

![](../images/part32/15.png)

Теперь желаемое количество равно 1. И на вкладке `Activity`, если мы обновим
`Activity history`, то увидим новую запись о запуске нового EC2 инстанса.

![](../images/part32/16.png)

Это может занять некоторое время. Итак, давайте обновим список. Теперь 
статус записи изменился на `MidLifecycleAction`.

![](../images/part32/17.png)

Подождем немного и снова обновим. На этот раз статус записи `Successful`.
Итак, теперь там доступен 1 инстанс в группе узлов.

![](../images/part32/18.png)

Давайте вернёмся на страницу группы узлов EKS кластера. Выберите
вкладку `Nodes` и нажмите эту кнопку, показанную на рисунке, для 
обновления.

![](../images/part32/19.png)

На этот раз в группе будет один узел. Но его статус по-прежнему не готов.
Нам нужно немного подождать, пока запустится EC2 инстанс.

![](../images/part32/20.png)

Вот сейчас как видно на рисунке узел готов к работе.

![](../images/part32/21.png)

Вернёмся в `k9s` консоль, чтобы посмотреть что произошло с подами. Он все 
ещё находится в режиме ожидания. Давайте узнаем, почему! Сейчас в списке
`Events` появилось два новых события.

![](../images/part32/22.png)

В первом говорится "0/1 nodes are available, 1 node had taint: not
ready, that the pod didn't tolerate", а во втором: "Too many pods".
Таким образом, это означает, что кластер распознал новый узел, процесс
развертывания попытался развернуть новый под на этом узле, но каким-то 
образом на узле уже запущено слишком много подов. Давайте копнем глубже, 
чтобы узнать, почему. Я открою список узлов.

![](../images/part32/23.png)

Это единственный узел кластера. Давайте выведем подробную информацию о нём!
Если мы прокрутим немного вниз до раздела `Capacity`, то увидим некоторые 
аппаратные характеристики узла, а именно количество ЦП и памяти.

![](../images/part32/24.png)

А внизу указано максимальное количество подов, которые могут работать на
этом узле, в нашем случае четыре. Также есть раздел, где указывается 
размер ресурсов, которые могут быть выделены этому узлу.

И если мы прокрутим немного вниз,

![](../images/part32/25.png)

то увидим количество подов, работа которых не завершена. В настоящее время
запущено четыре пода. И они приведены в этой таблице на рисунке. Все четыре 
пода принадлежат пространству имен `kube-system`. Таким образом, четыре 
системных пода Kubernetes уже заняли все четыре доступных слота для подов 
на узле. Вот почему процесс развертывания не может создать новый для нашего 
контейнера. Если вы не знали, максимальное количество подов, которые могут 
работать на EC2 инстансе, зависит от количества эластичных сетевых 
интерфейсов (или ENI) и количества IP-адресов на ENI, разрешенных для этого 
инстанса. На этой [странице Github](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt)
Amazon дает нам формулу для вычисления максимального количества подов,
основываясь на этих числах. Оно равна: количеству ENI, умноженному на 
(количество IP-адресов для одного ENI - 1) плюс 2.

Существует также [страница документации](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI) Amazon,
которая предоставляет нам количество ENI и Ips на один ENI для каждого типа
инстанса. Если вы ещё помните, мы используем экземпляр `t3.micro` для 
нашей группы узлов, поэтому, согласно этой таблице, он имеет 2 ENI и 2 
IP-адреса на один ENI. Теперь, если мы подставим эти числа в формулу, то 
получим `2 * (2 - 1) + 2 = 4`, что является максимальным количество подов, 
которые могут работать на инстансе этого типа. Если вам лень заниматься 
математикой, вы можете просто найти `t3.micro` на этой [странице](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt).
На ней тоже видно, что четыре — это максимальное количество подов.

Итак, чтобы запустить хотя бы ещё один под на узле, нам понадобится более
мощный тип инстанса. Ресурса `t3.nano` также недостаточно для запуска более 
четырёх подов, но инстанс `t3.small` может запускать до 11 подов, поэтому 
его должно быть более чем достаточно для нашего приложения.

Хорошо, теперь нам нужно вернуться на страницу группы узлов Amazon EKS 
кластера. Как мы видим здесь, текущий тип инстанса этой группы узлов —
`t3.micro`.

![](../images/part32/26.png)

Попробуем изменить его на `t3.small`. На этой странице редактирования
группы узлов мы можем изменить сразу несколько параметров, такие как
настройки масштабирования, Kubernetes labels, Kubernetes taints, Tags или 
конфигурацию обновления группы узлов.

![](../images/part32/27.png)

Но здесь нет возможности изменить тип инстанса группы узлов.

Поэтому я думаю, что нам нужно удалить эту группу узлов и создать новую.
Давайте сделаем это! Чтобы удалить эту группу узлов, мы должны ввести ее 
название как показано здесь на рисунке для подтверждения. Затем нажмите эту 
кнопку `Delete`.

![](../images/part32/28.png)

Хорошо, теперь, если мы вернемся на страницу кластера, мы увидим, что статус 
группы узлов изменился на `Deleting` («В процессе удаления»).

![](../images/part32/29.png)

Через несколько минут мы можем обновить страницу. Теперь старой группы 
больше нет. 

![](../images/part32/30.png)

Давайте нажмем кнопку `Add Node Group`, чтобы создать новую. Я буду 
использовать то же название `simple-bank` для этой группы узлов. Выберите
`AWSEKSNodeRole`, которую мы создали на предыдущих лекциях.

![](../images/part32/31.png)

Затем прокрутите вниз до конца и нажмите `Next`.

![](../images/part32/32.png)

Для конфигурации группы узлов мы будем использовать значения по умолчанию: 
`Amazon Linux 2` для типа образа и `On-demand` для `Capacity type`.
Но для типа инстанса мы выберем `t3.small` вместо `t3.micro`, как раньше.
Здесь мы видим, что максимальное значение ENI равно трём, а максимальное 
число IP адресов равно 12.

![](../images/part32/33.png)

Таким образом, максимальное количество подов равно этому максимальному 
количеству IP-адресов минус 1. Далее, размер диска, давайте установим равным
10 ГБ.

![](../images/part32/34.png)

Затем для масштабирования группы узлов я установлю минимальное количество
в 0, а желаемое количество равным одному узлу.

После этого давайте перейдём к следующему шагу.

![](../images/part32/35.png)

Здесь уже выбраны подсети по умолчанию, поэтому я буду использовать их.
Не нужно ничего менять.

![](../images/part32/36.png)

На последнем шаге мы можем просмотреть все настроенные параметры для 
группы узлов. И если всё в порядке, мы можем приступить к созданию группы.

![](../images/part32/37.png)

Итак, группа создается. Это может занять некоторое время. Поэтому пока мы 
ждём, вернёмся в консоль `k9s` и удалим существующий процесс развёртывания.

![](../images/part32/38.png)

Для этого достаточно просто нажать `Ctrl + d`. Затем выберите OK и нажмите
`Enter`. После этого процесс развёртывания будет удален. Под, которым он 
управлял, также удаляется.

После этого это окно снова будет пустым и готовы к новому процессу 
развёртывания. Теперь давайте обновим страницу, чтобы посмотреть, готова ли
к работе группа узлов или нет. Итак, теперь её статус `Active`, поэтому она
должна быть готова.

![](../images/part32/39.png)

Давайте откроем терминал и выполним команду `kubectl apply`, чтобы 
развернуть наше приложение.

```shell
kubectl apply -f eks/deployment.yaml
deployment.apps/simple-bank-api-deployment created
```

Развертывание создано. Давайте проверим так ли это в `k9s` консоли.

![](../images/part32/40.png)

Ура, я думаю что всё заработало, потому что на этот раз цвет изменился с 
красного на зеленый, и надпись изменила на `READY 1/1`.

![](../images/part32/41.png)

Давайте выведем подробную информацию о нём. Вроде бы всё в порядке. 
Количество реплик равно 1.

![](../images/part32/42.png)

Давайте посмотрим что с подами.

![](../images/part32/43.png)

На рисунке показан один под и со статусом `Running` («Выполняется»).
Превосходно!

Давайте выведем подробную информацию для него. Прокрутите в самый низ.
Мы видим несколько обычных событий. Все они успешно завершены. Контейнер 
создан и запущен без ошибок.

![](../images/part32/44.png)

Теперь, если мы вернемся к списку модулей и нажмем `Enter`, то перейдём в 
список контейнеров.

![](../images/part32/45.png)

Таким образом, в поде запущен только один контейнер. Если мы хотим 
посмотреть логи этого контейнера, то можем просто нажать L, как здесь 
показано на рисунке.

![](../images/part32/46.png)

Здесь на рисунке видны логи.

![](../images/part32/47.png)

Сначала в контейнере запускаются миграции БД, затем успешно запускается 
приложение. После этого сервер прослушивает и HTTP-запросы через порт `8080`. 
Замечательно!

Но возникает следующий вопрос: как мы можем отправлять запросы к этому API?
Если мы вернемся к списку подов, то увидим его IP-адрес.

![](../images/part32/48.png)

Однако это всего лишь внутренний IP-адрес, к которому нельзя получить доступ
извне кластера. Чтобы направить трафик из внешнего мира в под, нам нужно 
развернуть еще один объект Kubernetes, которым будет `Service` («Сервис»).
Подробнее об этом можно прочитать на [официальной странице документации 
Kubernetes](https://kubernetes.io/docs/concepts/services-networking/service/).
По сути, сервис — это абстракция, которая определяет набор правил для 
маршрутизации сетевого трафика к правильному приложению, работающему на 
наборе подов. Балансировка нагрузки между ними будет выполняться 
автоматически, поскольку все поды одного развертывания будут использовать 
один внутренний DNS. Ниже показан пример как мы можем определить сервис. 

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

Я скопирую его. Затем вернусь к коду. Давайте создадим новый файл:
`service.yaml` внутри папки `eks`. Затем вставьте скопированное содержимое из
примера сервиса.

Пример также начинается с версии API, как и для процесса развёртывания. Но
теперь вид этого объекта - `Service`.

```yaml
apiVersion: v1
kind: Service
```

У нас также существует раздел метаданных для хранения некоторой информации 
об этом сервисе. Здесь я просто укажу имя — `simple-bank-api-service`.

```yaml
metadata:
  name: simple-bank-api-service
```

Далее идёт спецификация сервиса. Во-первых, мы должны определить правило
выбора пода, чтобы сервис мог найти набор подов для перенаправления трафика.
Мы будем использовать метки для выбора подов, поэтому я скопирую метку 
приложения из раздела `template` в файле `deployment.yaml` и вставлю её в
файл `service.yaml` здесь, в раздел `selector`.

```yaml
spec:
  selector:
    app: simple-bank-api
```

Далее мы должны определить правило для портов. Этот сервис будет прослушивать
HTTP запросы к API, поэтому используется протокол `TCP`, затем `80` — это 
порт, на котором сервис будет прослушивать входящие запросы. И, наконец,
`targetPort` — это порт контейнера, на который будут отправляться запросы.
В нашем случае он равен `8080`, как мы указали в файле `deployment`. Поэтому 
я изменю это значение из файла примера на `8080` в файле `service`.

```yaml
spec:
  selector:
    app: simple-bank-api
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

Вот и все! Файл `service.yaml` готов. Теперь давайте откроем терминал и 
выполним `kubectl apply -f eks/service.yaml`, чтобы развернуть его.

```shell
kubectl apply -f eks/service.yaml
service/simple-bank-api-service created
```

Давайте найдём сервис `k9s` в консоли. Я поищу там по ключевому слову 
`services`.

![](../images/part32/49.png)

Вот что получим в результате. В списке сервисов помимо системного сервиса 
Kubernetes, мы видим наш `simple-bank-api-service`. Его тип `ClusterIP`, а 
также на рисунке показан внутренний IP-адрес кластера. И он прослушивает 
порт `80`, как мы указали в `yaml` файле. Но посмотрите на столбец 
`EXTERNAL-IP` («Внешний IP адрес»)! Он пустой! Это означает, что у этого
сервиса нет внешнего IP. Итак, как мы можем получить к нему доступ извне?
Что ж, чтобы сервис был доступен внешнему миру, нам нужно изменить его тип.
По умолчанию, если мы ничего не указываем, тип сервиса будет `ClusterIP`.
Теперь давайте изменим его тип на `LoadBalancer`.

```yaml
spec:
  selector:
    app: simple-bank-api
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer
```

Затем сохраните файл и вернитесь в терминал, чтобы добавить его снова.

```shell
kubectl apply -f eks/service.yaml
service/simple-bank-api-service configured
```

Итак, сервис настроен. На этот раз в консоли `k9s` мы видим, что его тип 
изменился на `LoadBalancer`, а также появился внешний IP, либо сервису 
присвоено доменное имя.

![](../images/part32/50.png)

Потрясающе! Но чтобы убедиться в правильности работы, давайте запустим
`nslookup` для этого домена.

![](../images/part32/51.png)

Мы получили ошибку: "server can't find this domain name" («сервер не может 
найти это доменное имя»).

Возможно необходимо подождать какое-то время, пока домен не будет готов к 
использованию. После этого давайте снова воспользуемся командой `nslookup`.

![](../images/part32/52.png)

На этот раз команда успешно выполнилась. На рисунке видно, что с этим 
доменом связаны 2 IP-адреса. Причина в том, это 
балансировщик сетевой нагрузки AWS.

## Посылаем запросы на сервер

Хорошо, теперь давайте попробуем отправить несколько запросов на сервер!

Я открою Postman и протестирую API для входа пользователя в систему. Мы 
должны заменить URL-адрес `localhost:8080` на доменное имя сервиса на 
продакшене. И если я правильно помню, мы уже создали пользователя `Алиса` в
продакшен БД в одной из предыдущих лекций. Я могу это быстро проверить с
помощью TablePlus.

![](../images/part32/53.png)

Да, всё верно! Пользователь `Alice` уже существует. Итак, вернемся в Postman 
и отправим запрос.

![](../images/part32/54.png)

Ура! Он успешно выполнен. Мы получили токен доступа со всей информацией о
пользователе. То есть всё сработало как надо! Теперь давайте посмотрим логи 
контейнера.

![](../images/part32/55.png)

Вот на рисунке мы видим POST-запрос к `/users/login`. Хорошо, а теперь 
вернемся к сервису и просмотрим информацию о нём.

![](../images/part32/56.png)

Мы видим, что он отправляет запрос только на одну единственную конечную 
точку. Это связано с тем, что мы создали только одну реплику приложения.
Давайте посмотрим, что произойдет, если мы изменим количество реплик на две 
в файле `deployment.yaml`. Сохраните его и выполните `kubectl apply` в 
терминале для повторного развертывания в кластере.

```shell
kubectl apply -f eks/deployment.yaml
deployment.apps/simple-bank-api-deployment configured
```

Хорошо, теперь давайте перейдем в список `Deployments`. На этот раз мы видим
READY 2/2, что означает, что существует две запущенных и работающих реплики 
или два пода.

![](../images/part32/57.png)

А вот и они!

![](../images/part32/58.png)

Теперь давайте ещё раз просмотрим информацию о сервисе. Если мы откроем
`simple-bank` API сервис,

![](../images/part32/59.png)

![](../images/part32/60.png)

то мы увидим, что теперь он перенаправляет запросы на 2 разные конечные 
точки, и эти конечные точки на самом деле являются адресами двух подов, 
на которых работает наше приложение.

Хорошо, давайте вернемся в Postman и снова отправим запрос, чтобы убедиться,
что он все ещё выполняется без ошибок.

![](../images/part32/61.png)

Здорово! Запрос успешно выполнен. Даже если я пошлю его несколько раз. Таким 
образом, сервис хорошо справляется с балансировкой нагрузки запроса при 
наличии нескольких подов.

Теперь, прежде чем мы закончим, давайте просмотрим ресурсы узла. Я выведу
подробную информацию об этом узле и прокручу вниз до раздела `Capacity`.

![](../images/part32/62.png)

Здесь мы видим, что максимальное количество подов, которые могут работать 
на этом узле, равно одиннадцати, что совпадает со значением, рассчитанным
для инстанса `t3.small`. И если мы прокрутим немного вниз, мы увидим, что 
на данный момент на этом узле работает 6 подов.

![](../images/part32/63.png)

Четыре из них — это системные поды Kubernetes. А два других — это наши поды
для развертывания API `simple-bank`.

Итак, это всё, чем я хотел поделиться с вами на сегодняшней лекции. 
Мы узнали как развернуть вею-приложение в Kubernetes кластере на AWS. И мы 
смогли отправить запросы к сервису из-за пределов кластера через внешний 
IP-адрес или автоматически сгенерированное доменное имя сервиса. Но, конечно, 
мы не хотим использовать такой домен для интеграции с фронтендом внешних 
сервисов, не так ли? Чего мы хотели бы добиться, так это иметь возможность 
связать сервис с определённым доменным именем, которое мы купили, например,
`simplebank.com` или подобным, верно?

Это будет темой следующей лекции. Надеюсь вам понравился материал, 
изложенный в этой лекции. Большое спасибо за время, потраченное на чтение!
Желаю вам получать удовольствие от обучения и до встречи на следующей лекции!